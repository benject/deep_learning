{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyORDyavtlJa3WBNmXpck6Eo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benject/deep_learning/blob/main/logistic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "logistic python implement"
      ],
      "metadata": {
        "id": "NG_M5f8FOnhd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "Qbg1yZ1_NxkG"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "\n",
        "  '''sigmoid 激活函数'''\n",
        "\n",
        "  return(1.0/(1.0 + np.exp(-z)))\n",
        "\n",
        "\n",
        "def loss( real_result , predict_result):\n",
        "\n",
        "  '''损失函数'''\n",
        "\n",
        "  result = -( real_result*np.log(predict_result) + (1 - real_result)*np.log(1-predict_result))\n",
        "\n",
        "  return result\n",
        "\n",
        "def gradient_w(input,real_result,predict_result):\n",
        "\n",
        "  '''\n",
        "  梯度下降\n",
        "  第一步 令 y = loss（a） 对损失函数的求导 dy/da 的结果是 -y/a + (1-y)/(1-a)\n",
        "  计算过程参见 https://towardsdatascience.com/logistic-regression-from-scratch-69db4f587e17\n",
        "\n",
        "  第二步 令 a = sigmoid（z） 对sigmoid函数求导 da/dz 的结果是 a(1-a)\n",
        "  计算过程参见 https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e\n",
        "\n",
        "  第三步 根据链式求导法则 dydz = dy/da * da/dz = (-y/a + (1-y)/(1-a)) * a(1-a) = -y(1-a) + a(1-y) = -y + ay + a - ay = a - y\n",
        "\n",
        "  第四步 令 z = w1x1 + w2x2 + b 对函数求w1的偏导数 dz/dw1 = x1 以此类推\n",
        "\n",
        "  那么： dy/dw1 = dz * dz/dw1 = x1 * (a - y) \n",
        "\n",
        "  可以发现 由于精妙的激活函数和损失函数的设计，预测值对参数值的梯度 就是 预测值与真值的差 再乘以输入\n",
        "  '''\n",
        "  \n",
        "  return( input *(predict_result - real_result) )\n",
        "\n",
        "def gradient_b(real_result,predict_result):  \n",
        "  \n",
        "  return( predict_result - real_result )\n",
        "\n",
        "class MyNeuron:\n",
        "\n",
        "  def __init__(self,w,b):\n",
        "\n",
        "    self.w = w\n",
        "    self.b = b\n",
        "    self.learning_rate = 0.05\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    z = np.dot(self.w,x) + b\n",
        "    return sigmoid(z)\n",
        "\n",
        "  def update_w(self,dw):\n",
        "\n",
        "    self.w = self.w - self.learning_rate * dw\n",
        "  \n",
        "  def update_b(self,b):\n",
        "\n",
        "    self.b = self.b - self.learning_rate * db\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = np.array([0.35,0.45])\n",
        "b = 1.31\n",
        "\n",
        "\n",
        "\n",
        "neu = MyNeuron (w.T,b)\n",
        "\n",
        "\n",
        "X = np.ndarray([60,2])\n",
        "Y = np.ndarray([60,1])\n",
        "\n",
        "for i in range(30):\n",
        "\n",
        "  X[i][0] = np.random.uniform() * 0.1 + 1.8\n",
        "  X[i][1] = np.random.uniform() * 0.001 + 0.085\n",
        "  Y[i] = 1.0\n",
        "\n",
        "for i in range(30):\n",
        "\n",
        "  X[30+i][0] = np.random.uniform() * 0.1 + 1.5\n",
        "  X[30+i][1] = np.random.uniform() * 0.001 + 0.045\n",
        "  Y[30+i] = 1.0\n",
        "\n",
        "y = 0 \n",
        "dw1 = 0 \n",
        "dw2 = 0\n",
        "db=0\n",
        "j = 0\n",
        "\n",
        "for i in range(60):\n",
        "\n",
        "  y = neu.forward(X[i])\n",
        "  dw1 += gradient_w(X[i][0],Y[i],y)\n",
        "  dw2 += gradient_w(X[i][1],Y[i],y)\n",
        "  db += gradient_b(Y[i],y)\n",
        "  j += loss(Y[i],y)\n",
        "\n",
        "dw1 /= 60\n",
        "dw2 /= 60\n",
        "db /= 60\n",
        "j /= 60\n",
        "\n",
        "neu.update_w(np.array([dw1,dw2]).T)\n",
        "neu.update_b(np.array(b))\n",
        "\n",
        "print(neu.w)\n",
        "print(neu.b)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTIM2CRZRGI_",
        "outputId": "501d032f-b664-4933-867b-543070424dc0"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.36071948 0.45040602]]\n",
            "[1.31630352]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8k6FywQIUwZb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}